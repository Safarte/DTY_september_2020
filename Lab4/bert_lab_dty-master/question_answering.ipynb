{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W-1zl5XdYInf"
   },
   "source": [
    "# BERT lab 4: Question Answerin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Su7fixBdiUex"
   },
   "source": [
    "### The SQuAD v1.1 Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bT5ESKDxfnLf"
   },
   "source": [
    "\"Question Answering\" task is the application BERT to the Stanford Question Answering Dataset (SQuAD). The task posed by the SQuAD benchmark is a little different than you might think. Given a question, and **a passage of text containing the answer**, BERT needs to highlight the \"span\" of text corresponding to the correct answer. \n",
    "\n",
    "The SQuAD homepage has a fantastic tool for exploring the questions and reference text for this dataset, and even shows the predictions made by top-performing models. For example, here are some [interesting examples](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/Super_Bowl_50.html?model=r-net+%20(ensemble)%20(Microsoft%20Research%20Asia)&version=1.1).\n",
    "\n",
    "In this last kab, we'll be downloading a model that's *already been fine-tuned* for question answering, and try it out on our own text. If you do want to fine-tune on your own dataset, it is possible to fine-tune BERT for question answering yourself. See [run_squad.py](https://github.com/huggingface/transformers/blob/master/examples/run_squad.py) example.\n",
    "\n",
    "**Note:** The example code in this lab is a commented and expanded version of the short example provided in the `transformers` documentation [here](https://huggingface.co/transformers/model_doc/bert.html?highlight=bertforquestionanswering#transformers.BertForQuestionAnswering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xN5f1bxf6K_"
   },
   "source": [
    "### Input Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ctum5SK6f9uP"
   },
   "source": [
    "For a Question Answering task, we need to pack both the question and the reference that contains the answer into a single continus input, where the two are seperated by a `[SEP]` token.\n",
    "\n",
    "<img src=\"https://i.imgur.com/qtwZvtb.png\" width=\"800\">\n",
    "\n",
    "The two pieces of text are separated by the special `[SEP]` token. The sentence embeddings (or segment embeddings) are also used to differentiate the question from the reference text. These are simply two embeddings (for sentences \"A\" and \"B\") that BERT learned, and which it adds to the token embeddings before feeding them into the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xs31dcrPg5Tg"
   },
   "source": [
    "### Start & End Token Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvOdUa9Wg-Uv"
   },
   "source": [
    "In order to detect the span of the reference test that contains the anwser, BERT simply needs to predict which token marks the start of the answer, and which token marks the end.\n",
    "\n",
    "To his end, we'll have two classifiers, a start classifier that will give the probability the a given token is the start of the anwer, which a simple dot product with a vector of the same size as the output embeddings followed by a softmax to get a probability distribution. So it is like we compare all of the output embeddings with the embeddings of a `[START]` embedding. The same is done for the `[END]`. The results are the position with the maximum probability for start and end.\n",
    "\n",
    "<img src=\"https://i.imgur.com/aLf7jLu.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVq-TuylYRDW"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9nhy3PzGQ44"
   },
   "source": [
    "As always, let's set the GPU and the get the transformer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQl0MMrOGIup"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "-ONLrgJK99TQ",
    "outputId": "39eb2654-1047-4508-898f-a0d305ff2a48"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    assert False, \"Please select GPU in the Colab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WThOUtpYvG-"
   },
   "source": [
    "### Load Fine-Tuned BERT-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaweLnNXGhTY"
   },
   "source": [
    "For Question Answering we'll use the `BertForQuestionAnswering` class from the `transformers` library.\n",
    "This class supports fine-tuning, but for this example we will keep things simpler and load a BERT model that has already been fine-tuned for the SQuAD benchmark.\n",
    "\n",
    "The `transformers` library has a large collection of pre-trained models which you can reference by name and load easily. The full list is in their documentation [here](https://huggingface.co/transformers/pretrained_models.html).\n",
    "\n",
    "For Question Answering, they have a version of BERT-large that has already been fine-tuned for the SQuAD benchmark. BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters. Instead of the base version, with 12 layers and embedding of 768. Totaling 1.34GB, so expect it make take a couple minutes to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Mnv95sX-U9K"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8imoOxoqGZ0h"
   },
   "source": [
    "Load the tokenizer as well. The vocabulary of BERT large is identicaly to the one in bert-base-uncased. So we can load the tokenizer from `bert-base-uncased` and that works just as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFQ5f7gv-RBH"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I__1ubvcZYow"
   },
   "source": [
    "### Asking a Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8MQ7b-GJIcM"
   },
   "source": [
    "Now we're ready to feed in an example! A QA example consists of a question and a passage of text containing the answer to that question.\n",
    "\n",
    "Let's try an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWzZP4EN-Zxg"
   },
   "outputs": [],
   "source": [
    "question = \"What year did the first manned Apollo flight occur?\"\n",
    "answer_text = \"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project Gemini (1962–66). The first manned flight of Apollo was in 1968..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llLvxhScKLZn"
   },
   "source": [
    "##  <span style=\"color:red\">Your turn. </span>\n",
    "\n",
    "1. **Tokenize both the question and the answer text**\n",
    "1. **Concatenate them, and place the special token `[SEP]` in between**\n",
    "1. **Create segment IDs; a simple list, with 0s for the first sentence & `[SEP]`, which is the question in our case, and 1s for the rest. Must be of the same length as the input.**\n",
    "1. **Conver the inputs to pytorch tensors.**\n",
    "1. **Pass them to the model, the segment ids are passed as argument named `token_type_ids`.**\n",
    "1. **With two outputs `start_scores, end_scores = model()`. Get the starting and ending positions.**\n",
    "1. **Using the start and end positions, convert the ids tokens to tokens using `tokenizer.convert_ids_to_tokens`, then print the answer with the correct formatting.**\n",
    "1. **Plot the output distributions for START and END positions£.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYoX33CfKGsr"
   },
   "outputs": [],
   "source": [
    "# Your turn"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "question_answering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
